{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imagenes/rn3.png\" width=\"200\">\n",
    "<img src=\"http://www.identidadbuho.uson.mx/assets/letragrama-rgb-150.jpg\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [Curso de Redes Neuronales](https://curso-redes-neuronales-unison.github.io/Temario/)\n",
    "\n",
    "# Una red neuronal convolucional simple \n",
    "\n",
    "\n",
    "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/), 2 de octubre de 2018.\n",
    "\n",
    "\n",
    "\n",
    "En esta libreta se muestra el ejemplo básico para una red convolucional\n",
    "aplicada al conjunto de datos [MNIST](http://yann.lecun.com/exdb/mnist/).\n",
    "La estructura de la red que vamos a hacer está inspirada (y es básicamente la misma) que la arquitectura conocisa como [LeNet 5](https://engmrk.com/lenet-5-a-classic-cnn-architecture/), la cual fue una de las primeras redes neuronales convolucionales exitosas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar datos\n",
    "\n",
    "Primero cargamos los archivos que se utilizan para el aprendizaje. Para otro tipo de problemas, es necesario hacer un proceso conocido como *Data Wrangling*, que normalmente se realiza con la ayuda de *Pandas*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que un aprendizaje tenga sentido es necesario tener bien separado un conjunto de datos de aprendizaje y otro de prueba (en caso de grandes conjuntos de datos es la opción). Como vemos tanto las imágenes como las etiquetas están separados en archivos de datos y de aprendizaje.\n",
    "\n",
    "El objeto `mnist` es un objeto tensorflow que contiene 3 objetos tipo tensorflow: *test*, *train* y *validation*, los cuales a su vez contienen *ndarrays* de *numpy*. La estructura es la misma para cada conjunto de datos. Veamos su estructura:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tipo de images: {}\".format(type(mnist.train.images)))\n",
    "print(\"Tipo de epochs_completed: {}\".format(type(mnist.train.epochs_completed)))\n",
    "print(\"Tipo de labels: {}\".format(type(mnist.train.labels)))\n",
    "print(\"Tipo de next_batch: {}\".format(type(mnist.train.next_batch)))\n",
    "print(\"Tipo de num_examples: {}\".format(type(mnist.train.num_examples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como generar el conjunto de datos para ser utilizado dentro de TensorFlow es objeto de otra libreta. Por el momento concentremonos en como hacer una red neuronal rápido y sin dolor.\n",
    "\n",
    "Sin embargo, vamos a ver unos cuantos datos que nos pueden ser de útilidad para la construcción de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Forma del ndarray con las imágenes: {}\".format(mnist.train.images.shape))\n",
    "print(\"Forma del ndarray con las etiquetas: {}\".format(mnist.train.labels.shape))\n",
    "print(\"-\" * 79)\n",
    "print(\"Número de imagenes de entrenamiento: {}\".format(mnist.train.images.shape[0]))\n",
    "print(\"Tamaño de las imagenes: {}\".format(mnist.train.images.shape[1]))\n",
    "print(\"Clases diferentes: {}\".format(mnist.train.labels.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construcción de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer una red neuronal lo más genérica posible y que podamos reutilizar en otros proyectos, vamos a establecer los parámetros base independientemente de la inicialización de la red, independientemente de la forma en que construimos la red. \n",
    "\n",
    "Comencemos por establecer una función genérica que nos forme una red neuronal con dos capas convolucionales y una capa oculta densa posterior. No agrego más comentarios porque, con la experiencia de las libretas anteriores, la construcción de la red neuronal se explica sola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_mnist(x, pesos, sesgos, dropout):\n",
    "    \"\"\"\n",
    "    Genera una red neuronal de dos capas para usar en TensorFlow\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    pesos: un diccionario con cuatro etiquetas: 'c1', 'c2', w1' y 'wo'\n",
    "           en donde cada una es una tf.Variable conteniendo una \n",
    "           matriz con los pesos tanto de los filtros convolucionales 'c1' y'c2'\n",
    "           como de las capas densas 'w1' y 'wo'\n",
    "                  \n",
    "    sesgos: un diccionario con cuatro etiquetas: 'b1', 'b2, 'b3' y 'bo'\n",
    "            en donde cada una es una tf.Variable conteniendo un\n",
    "            vector de dimensión [numero_de_neuronas_capa]\n",
    "            \n",
    "    dropout: Un flotante 0 <= dropout <= 1 con el porcentaje de dropout \n",
    "             en la capa densa oculta\n",
    "                   \n",
    "    Devuelve\n",
    "    --------\n",
    "    Un ops de tensorflow que calcula la salida de una red neuronal\n",
    "    con dos capas convolucionales, una capa oculta, y activaciones RELU.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Ajusta el vector de entrada de (-1, 784) que son los ejemplos que entran\n",
    "    # a la red para reconocimiento y/o aprendizaje, a un tensor de dimensión\n",
    "    # [-1, 28, 28, 1] (el último 1 implica que es una imagen BW, o una sola capa)\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "       \n",
    "    # Primera capa convolucional con activación ReLU y max_pool de 2X2\n",
    "    capa_1 = tf.nn.conv2d(x, pesos['c1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    capa_1 = tf.nn.bias_add(capa_1, sesgos['b1'])\n",
    "    capa_1 = tf.nn.relu(capa_1)\n",
    "    capa_1m = tf.nn.max_pool(capa_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    # Segunda capa convolucional con activación ReLU\n",
    "    capa_2 = tf.nn.conv2d(capa_1m, pesos['c2'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    capa_2 = tf.nn.bias_add(capa_2, sesgos['b2'])\n",
    "    capa_2 = tf.nn.relu(capa_2)\n",
    "    capa_2m = tf.nn.max_pool(capa_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Desenrollamos las imágenes para poder aplicar capas densas\n",
    "    capa_2desen = tf.reshape(capa_2m, [-1, pesos['w1'].get_shape().as_list()[0]])\n",
    "    \n",
    "    # Capa oculta densa con activación ReLU\n",
    "    capa_3 = tf.matmul(capa_2desen, pesos['w1'])\n",
    "    capa_3 = tf.add(capa_3, sesgos['b3'])\n",
    "    capa_3 = tf.nn.relu(capa_3)\n",
    "    \n",
    "    # Se le agrega Dropout por la generalización\n",
    "    capa_3 = tf.nn.dropout(capa_3, dropout)\n",
    "\n",
    "    # Capa de salida con activación lineal\n",
    "    # En Tensorflow la salida es siempre lineal, y luego se especifica\n",
    "    # la función de salida a la hora de calcularla como vamos a ver \n",
    "    # más adelante\n",
    "    capa_salida = tf.matmul(capa_3, pesos['wo']) \n",
    "    capa_salida = tf.add(capa_salida, sesgos['bo'])\n",
    "    return capa_salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora necesitamos poder generar los datos de entrada a la red neuronal de\n",
    "alguna manera posible. Afortunadamente sabemos exactamente que necesitaos, así\n",
    "que vamos a hacer una función que nos genere las variables de peso y sesgo.\n",
    "\n",
    "Por el momento, y muy a la brava, solo vamos a generarlas con números aletorios con una \n",
    "distribución $\\mathcal{N}(0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializa_pesos(filtro1, filtro2, n1, n2, n3):\n",
    "    \"\"\"\n",
    "    Genera un diccionario con pesos  \n",
    "    para ser utilizado en la función red_neuronal_dos_capas_ocultas\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    filtro1: (x, y) tupla con el tamaño del primer filtro convolucional\n",
    "\n",
    "    filtro2: (x, y) tupla con el tamaño del segundo filtro convolucional\n",
    "\n",
    "    n1: Número de imágenes generadas con el primer filtro\n",
    "    \n",
    "    n2: Número de imagenes generadas con el segundo filtro\n",
    "    \n",
    "    n3: Número de neuronas en la capa oculta\n",
    "\n",
    "    Asumimos que es una red para MNIST, por lo que la entrada son imágenes de 28 x 28 x 1 (BW)\n",
    "    y que tenemos 10 valores de salida diferentes\n",
    "    \n",
    "    Como hacemos maxpool de 2x2, las imagenes despues de la primer capa convolucional seran de \n",
    "    14 x 14, y las imágenes que salgan de la segunda capa convolucional serán de 7 x 7\n",
    "    \n",
    "    Devuelve\n",
    "    --------\n",
    "    Dos diccionarios, uno con los pesos por capa y otro con los sesgos por capa\n",
    "    \n",
    "    \"\"\"\n",
    "    pesos = {\n",
    "        'c1': tf.Variable(tf.random_normal([filtro1[0], filtro1[1], 1, n1])),\n",
    "        'c2': tf.Variable(tf.random_normal([filtro2[0], filtro2[1], n1, n2])),\n",
    "        'w1': tf.Variable(tf.random_normal([7 * 7 * n2, n3])),\n",
    "        'wo': tf.Variable(tf.random_normal([n3, 10]))\n",
    "    }\n",
    "    \n",
    "    sesgos = {\n",
    "        'b1': tf.Variable(tf.random_normal([n1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n3])),\n",
    "        'bo': tf.Variable(tf.random_normal([10]))\n",
    "    }\n",
    "    \n",
    "    return pesos, sesgos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos establecer los parámetros de la topología de la red neuronal. \n",
    "Tomemos en cuenta que estos prámetros los podríamos haber establecido desde\n",
    "la primer celda, si el fin es estar variando los parámetros para escoger los que \n",
    "ofrezcan mejor desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entradas = 784  #  Lo sabemos por la inspección que hicimos a mnist\n",
    "num_salidas = 10    # Ídem\n",
    "\n",
    "# Aqui es donde podemos jugar\n",
    "filtro1 = (5, 5)\n",
    "filtro2 = (5, 5)\n",
    "n1 = 32\n",
    "n2 = 64\n",
    "n3 = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡A construir la red! Para esto vamos a necesitar crear las entradas\n",
    "con un placeholder, y crear nuestra topología de red neuronal.\n",
    "\n",
    "Observa que la dimensión de x será [None, num_entradas], lo que significa que \n",
    "la cantidad de renglones es desconocida (o variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La entrada a la red neuronal\n",
    "x = tf.placeholder(\"float\", [None, num_entradas])\n",
    "\n",
    "# Los pesos y los sesgos\n",
    "w, b = inicializa_pesos(filtro1, filtro2, n1, n2, n3) \n",
    "\n",
    "# El valor de dropout que puede variar en el tiempo\n",
    "dp = tf.placeholder(tf.float32)\n",
    "\n",
    "# Crea la red neuronal\n",
    "estimado = cnn_mnist(x, w, b, dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parecería que ya está todo listo. Sin ambargo falta algo muy importante: No hemos explicado\n",
    "ni cual es el criterio de error (loss) que vamos a utilizar, ni cual va a ser el método de\n",
    "optimización (aprendizaje) que hemos decidido aplicar.\n",
    "\n",
    "Primero definamos el costo que queremos minimizar, y ese costo va a estar en función de lo\n",
    "estimado con lo real, por lo que necesitamos otra entrada de datos para los datos de salida.\n",
    "\n",
    "Sin ningun lugar a dudas, el costo que mejor describe este problema es el de *softmax*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Creamos la variable de datos de salida conocidos\n",
    "y = tf.placeholder(\"float\", [None, num_salidas])\n",
    "\n",
    "#  Definimos la función de costo\n",
    "costo = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=estimado, labels=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora definimos que función de aprendizaje vamos a utilizar. Existen muchas funciones\n",
    "de aprendizaje en tensorflow, las cuales se pueden consultar en `tf.train.`. Entre las\n",
    "existentes podemos ver algunas conocidas del curso como descenso de gradiente simple,\n",
    "momento, rprop, rmsprop entre otras. Casi todas las funciones de optimización (aprendizaje)\n",
    "acaban su nombre con `Optimize`.\n",
    "\n",
    "En este caso vamos a usar un método comocido como el *algoritmo de Adam* el cual \n",
    "se puede consultar [aqui](http://arxiv.org/pdf/1412.6980.pdf). El metodo utiliza dos calculos\n",
    "de momentos diferentes, y por lo visto genera resultados muy interesantes desde el punto \n",
    "de vista práctico.\n",
    "\n",
    "¿Cual es el mejor método? Pues esto es en función de tu problema y de la cantidad de datos que tengas.\n",
    "Lo mejor es practicar con varios métodos para entender sus ventajas y desventajas.\n",
    "\n",
    "En todo caso el método de optimización requiere que se le inicialice con una tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alfa = 0.001\n",
    "optimizador = tf.train.AdamOptimizer(learning_rate=alfa)\n",
    "paso_entrenamiento = optimizador.minimize(costo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ejecutar la sesión usando mini-batches\n",
    "\n",
    "Ahora, ya que la red neuronal está lista vamos a ejecutar la red utilizando el algoritmo de\n",
    "Adam pero en forma de mini-batches. Con el fin de tener control sobre el problema, vamos a establecer un número máximo de epoch (ciclos de aprendizaje), el tamaño de los mini-batches, y cada cuandos epoch \n",
    "quisieramos ver como está evolucionando la red neuronal.\n",
    "\n",
    "Como entrenar una red neuronal no tiene sentido, si no es porque la queremos usar para reconocer,\n",
    "no tendría sentido entrenarla y luego perderla y tener que reentrenar en cada ocasión. Recuerda que cuando\n",
    "se cierra la sesión se borra todo lo que se tenía en memoria. \n",
    "\n",
    "Para esto vamos a usar una ops especial llamada `Saver`, que permite guardar en un archivo la red neuronal y \n",
    "después utilizarla en otra sesión (en otro script, computadora, ....).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_modelo = \"simple/cnn_simple\"\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como todo se ejecuta dentro de una sesión, no es posible hacerlo por partes (si usamos el \n",
    "`with` que debería ser la única forma en la que iniciaramos una sesión). Por lo tanto procuraré dejar comentado el código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numero_epochs = 5\n",
    "tamano_minibatch = 100\n",
    "display_step = 1\n",
    "dropout = 0.5\n",
    "\n",
    "# Muy importante la primera vez que se ejecuta inicializar todas las variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# La manera correcta de iniciar una sesión y realizar calculos\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Ciclos de entrenamiento\n",
    "    for epoch in range(numero_epochs):\n",
    "\n",
    "        #  Inicializa el costo promedio de todos los minibatches en 0\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        #  Calcula el número de minibatches que se pueden usar \n",
    "        total_batch = int(mnist.train.num_examples/tamano_minibatch)\n",
    "\n",
    "        #  Por cada minibatch\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            #  Utiliza un generador incluido en mnist que obtiene \n",
    "            #  tamano_minibatch ejemplos selecionados aleatoriamente del total\n",
    "            batch_x, batch_y = mnist.train.next_batch(tamano_minibatch)\n",
    "            \n",
    "            #  Ejecuta la ops del paso_entrenamiento para aprender \n",
    "            #  y la del costo, con el fin de mostrar el aprendizaje\n",
    "            _, c = sess.run(\n",
    "                [paso_entrenamiento, costo], \n",
    "                feed_dict={x: batch_x, y: batch_y, dp: dropout}\n",
    "            )\n",
    "            \n",
    "            #  Calcula el costo del minibatch y lo agrega al costo total\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        # Muestra los resultados\n",
    "        if epoch % display_step == 0:\n",
    "            print ((\"Epoch: \" + str(epoch)).ljust(20)\n",
    "                   + (\"Costo: \" + str(avg_cost)))\n",
    "    \n",
    "        #  Guarda la sesión en el archivo rnn2.cptk\n",
    "        saver.save(sess, archivo_modelo, global_step=i)\n",
    "    \n",
    "    print(\"Se acabaron los epochs, saliendo de la sesión de tensorflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ahora vamos a revisar que tan bien realizó el aprendizaje cuando se aplica la red adatos que\n",
    "no se usaron para entrenamiento. Para esto vamos a utilizar dos ops extas: una \n",
    "para definir la operaración de datos bien estimados o mal estimados, y otra para\n",
    "calcular el promedio de datos bien estimados. Para calcular los datos bien estimados vamos a utilizar `tf.cast` que permite ajustar los tipos\n",
    "al tipo tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_correcta = tf.equal(\n",
    "    tf.argmax(estimado, 1), \n",
    "    tf.argmax(y, 1)\n",
    ")\n",
    "\n",
    "precision = tf.reduce_mean(tf.cast(prediction_correcta, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si, vamos a abrir una nueva sesión, vamos a restaurar los valores de la sesión anterior,\n",
    "y vamos a ejecutar el grafo con el fin de evaluar la ops precision, pero ahora con el\n",
    "diccionario de alimentación con los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, archivo_modelo)\n",
    "    porcentaje_acierto = sess.run(\n",
    "        precision, \n",
    "        feed_dict={\n",
    "            x: mnist.test.images,\n",
    "            y: mnist.test.labels,\n",
    "            dp: dropout\n",
    "        }\n",
    "    )\n",
    "    print(\"Precisión: {}\".format(porcentaje_acierto))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Contesta las siguientes preguntas\n",
    "\n",
    "1. ¿Cuanto tiempo toma en realizar un epoch en tu computadora? ¿Y en la GPU Titan?\n",
    "\n",
    "2. Realiza solamente 5 epoch cada vez y prueba con el conjunto de test para comprobar como \n",
    "   mejora el reconocimiento. Reinicia el aprendizaje desde donde te quedaste (recuperando la sesión) al menos 3 veces más.\n",
    "\n",
    "4. ¿Que pasa si cambias los tamaños de los filtros o el número de imágenes generadas?\n",
    "\n",
    "5. ¿Que pasa si cambias el tamaño de los minibatches?\n",
    "\n",
    "6. ¿Qué pasa si disminuyes el `dropout` a 0.25? ¿Qué pasa si aumentas el `dropout` a 0.75?\n",
    "\n",
    "**Para contestar las preguntas, agrega cuantas celdas con comentarios y con códgo sean necesarias.** Aprovecha que las libretas de *Jupyter* te permite hacerte una especie de tutorial personalizado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
